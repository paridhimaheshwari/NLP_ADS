{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-bZui4bEzna"
      },
      "source": [
        "### **Traditional (Rule-Based NLP using NLTK and Spacy)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "  Downloading spacy-3.7.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (25 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.0 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
            "Collecting thinc<8.3.0,>=8.1.8 (from spacy)\n",
            "  Downloading thinc-8.2.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (20 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
            "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/paridhimaheshwari/.virtualenvs/django_env/lib/python3.11/site-packages (from spacy) (4.66.1)\n",
            "Collecting requests<3.0.0,>=2.13.0 (from spacy)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
            "  Downloading pydantic-2.6.0-py3-none-any.whl.metadata (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting jinja2 (from spacy)\n",
            "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: setuptools in /Users/paridhimaheshwari/.virtualenvs/django_env/lib/python3.11/site-packages (from spacy) (67.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/paridhimaheshwari/.virtualenvs/django_env/lib/python3.11/site-packages (from spacy) (23.1)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-1.26.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pydantic-core==2.16.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading pydantic_core-2.16.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
            "Collecting typing-extensions>=4.6.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading urllib3-2.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.1.8->spacy)\n",
            "  Downloading blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.1.8->spacy)\n",
            "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/paridhimaheshwari/.virtualenvs/django_env/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
            "Downloading spacy-3.7.2-cp311-cp311-macosx_11_0_arm64.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl (26 kB)\n",
            "Downloading numpy-1.26.3-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.8/128.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.6.0-py3-none-any.whl (394 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.16.1-cp311-cp311-macosx_11_0_arm64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.4/488.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.2.2-cp311-cp311-macosx_11_0_arm64.whl (778 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.9/778.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Downloading blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
            "Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl (18 kB)\n",
            "Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Downloading urllib3-2.2.0-py3-none-any.whl (120 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.9/120.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cymem, wasabi, urllib3, typing-extensions, spacy-loggers, spacy-legacy, smart-open, numpy, murmurhash, MarkupSafe, langcodes, idna, cloudpathlib, charset-normalizer, certifi, catalogue, annotated-types, typer, srsly, requests, pydantic-core, preshed, jinja2, blis, pydantic, confection, weasel, thinc, spacy\n",
            "Successfully installed MarkupSafe-2.1.5 annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 certifi-2024.2.2 charset-normalizer-3.3.2 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 idna-3.6 jinja2-3.1.3 langcodes-3.3.0 murmurhash-1.0.10 numpy-1.26.3 preshed-3.0.9 pydantic-2.6.0 pydantic-core-2.16.1 requests-2.31.0 smart-open-6.4.0 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.2 typer-0.9.0 typing-extensions-4.9.0 urllib3-2.2.0 wasabi-1.1.2 weasel-0.3.4\n"
          ]
        }
      ],
      "source": [
        "# !pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2bysXLMHEkq6"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j7mkx9DE8NU"
      },
      "source": [
        "**1. Tokenization- The process of breaking a text down into tokens is called tokenization.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQfiJ-iDErDB",
        "outputId": "0e5bc76d-f8ba-4509-9aeb-d43a8ddde0ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['mary', ',', 'do', 'n’t', 'slap', 'the', 'green', 'witch']\n"
          ]
        }
      ],
      "source": [
        "#using spacy\n",
        "text = \"Mary, don’t slap the green witch\"\n",
        "print([str(token) for token in nlp(text.lower())])\n",
        "# nlp gives 'doc' objects,that is why conversion to str is required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['nsubj', 'punct', 'aux', 'neg', 'ROOT', 'det', 'amod', 'dobj']\n"
          ]
        }
      ],
      "source": [
        "print([token.dep_ for token in nlp(text.lower())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHamtVCFFf6R",
        "outputId": "0d8c82d4-0ed1-416d-9d84-be231dedf16a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Mary', ',', 'don', '’', 't', 'slap', 'the', 'green', 'witch']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#using nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Mary, don’t slap the green witch\"\n",
        "word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPbEVtF6GxbB",
        "outputId": "0ddb2c21-df1c-48e5-ac0e-f984cd9f8404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['snow', 'white', 'and', 'the', 'seven', 'degrees', '#makeamoviecold', '@midnight', ':', ')']\n"
          ]
        }
      ],
      "source": [
        "#Tokenizing tweets using NLTK\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet=\"Snow White and the Seven Degrees #MakeAMovieCold@midnight: )\"\n",
        "tokenizer = TweetTokenizer()\n",
        "print(tokenizer.tokenize(tweet.lower()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p2PzPVUH0T8"
      },
      "source": [
        " **2. Stopword Removal-Stopwords such as articles and\n",
        "prepositions serve mostly a grammatical purpose, like filler holding the content words.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYkb0MGcIAWi",
        "outputId": "b93e4b19-7816-43b6-90d8-d826bd990206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "we\n",
            "will\n",
            "show\n",
            "how\n",
            "to\n",
            "remove\n",
            "stopwords\n",
            "using\n",
            "spacy\n",
            "library\n",
            "['remove', 'stopwords', 'spacy', 'library']\n"
          ]
        }
      ],
      "source": [
        "#using spacy\n",
        "nlp = spacy.load('en_core_web_sm')              #didnt specify language since 'en' model has been loaded\n",
        "stopwords = nlp.Defaults.stop_words\n",
        "text = \" we will show how to remove stopwords using spacy library\"\n",
        "\n",
        "lst=[]\n",
        "for token in text.split():\n",
        "    print(token)\n",
        "    if token.lower() not in stopwords:    #checking whether the word is not\n",
        "        lst.append(token)                    #present in the stopword list.\n",
        "print(lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZmRFLziIrbD",
        "outputId": "77cb4676-af63-444b-e798-6e641663b14a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['show', 'remove', 'stopwords', 'using', 'spacy', 'library']\n"
          ]
        }
      ],
      "source": [
        "#using nltk\n",
        "from nltk.corpus import stopwords\n",
        "stopwords=stopwords.words('english')\n",
        "text = \" we will show how to remove stopwords using spacy library\"\n",
        "\n",
        "lst=[]\n",
        "for token in text.split():\n",
        "    if token.lower() not in stopwords:    #checking whether the word is not\n",
        "        lst.append(token)                    #present in the stopword list.\n",
        "print(lst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAsFn4dUKV2P"
      },
      "source": [
        "**3. Unigrams, Bigrams, Trigrams, …, N-grams**\n",
        "N grams are fixed length (n) consecutive token sequences occurring in the text. A bigram has two\n",
        "tokens, a unigram one. Generating n grams from a text is straightforward enough, but packages like spaCy and NLTK provide convenient methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbBCNzdnKBq_",
        "outputId": "c9c4392c-4b59-4ea6-a6bb-58e5313b1cc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['mary', ',', \"n't\", 'slap'], [',', \"n't\", 'slap', 'green'], [\"n't\", 'slap', 'green', 'witch'], ['slap', 'green', 'witch', '.']]\n"
          ]
        }
      ],
      "source": [
        "def n_grams(text, n):\n",
        "  return [text[i:i+n] for i in range(len(text)-n+1)]\n",
        "cleaned = ['mary', ',', \"n't\", 'slap', 'green', 'witch', '.']\n",
        "print(n_grams(cleaned, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUDCGNdvKUPe",
        "outputId": "bbb8a08c-63b5-4526-9993-bada9917f1d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['quark', 'soup', 'interact', 'localize', 'assembly', 'quark', 'gluon']\n",
            "['quark_soup', 'soup_interact', 'interact_localize', 'localize_assembly', 'assembly_quark', 'quark_gluon']\n"
          ]
        }
      ],
      "source": [
        "#using spacy\n",
        "import spacy\n",
        "from spacy_ngram import NgramComponent\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')  # or whatever model you downloaded\n",
        "nlp.add_pipe('spacy-ngram')  # default to document-level ngrams, removing stopwords\n",
        "# add_pipe meaning?? -> it's spacy's pipeline to which we have added 'spacy_ngram' for the given program usage.\n",
        "text = 'Quark soup is an interacting localized assembly of quarks and gluons.'\n",
        "\n",
        "print(nlp(text)._.ngram_1)\n",
        "print(nlp(text)._.ngram_2)\n",
        "# print(nlp(text)._.ngram_3)        ??? Can't retrieve unregistered extension attribute 'ngram_3'. Did you forget to call the `set_extension` method?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSo5Hdr4MruT",
        "outputId": "3e4a804e-01e6-4404-8793-d1841b68d279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Quark',)\n",
            "('soup',)\n",
            "('is',)\n",
            "('an',)\n",
            "('interacting',)\n",
            "('localized',)\n",
            "('assembly',)\n",
            "('of',)\n",
            "('quarks',)\n",
            "('and',)\n",
            "('gluons.',)\n",
            "('Quark', 'soup')\n",
            "('soup', 'is')\n",
            "('is', 'an')\n",
            "('an', 'interacting')\n",
            "('interacting', 'localized')\n",
            "('localized', 'assembly')\n",
            "('assembly', 'of')\n",
            "('of', 'quarks')\n",
            "('quarks', 'and')\n",
            "('and', 'gluons.')\n",
            "('Quark', 'soup', 'is', 'an')\n",
            "('soup', 'is', 'an', 'interacting')\n",
            "('is', 'an', 'interacting', 'localized')\n",
            "('an', 'interacting', 'localized', 'assembly')\n",
            "('interacting', 'localized', 'assembly', 'of')\n",
            "('localized', 'assembly', 'of', 'quarks')\n",
            "('assembly', 'of', 'quarks', 'and')\n",
            "('of', 'quarks', 'and', 'gluons.')\n"
          ]
        }
      ],
      "source": [
        "#using NLTK\n",
        "from nltk.util import ngrams\n",
        "text = 'Quark soup is an interacting localized assembly of quarks and gluons.'\n",
        "# nltk digests lists instead of text directly(like in spacy)\n",
        "unigrams = ngrams(text.split(), 1)\n",
        "for item in unigrams:\n",
        "    print(item)\n",
        "bigrams = ngrams(text.split(), 2)\n",
        "for item in bigrams:\n",
        "    print(item)\n",
        "\n",
        "quadgrams = ngrams(text.split(), 4)\n",
        "for item in quadgrams:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLfez0c-N2dY"
      },
      "source": [
        "**4. Lemmas and Stems**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLQL4S4KNjvw",
        "outputId": "3e08da9b-c762-400e-afdf-2bf4f47444c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "he > he\n",
            "was > be\n",
            "running > run\n",
            "late > late\n"
          ]
        }
      ],
      "source": [
        "#Lemmatization Using Spacy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"he was running late\")\n",
        "for token in doc:\n",
        "  print('{} > {}'.format(token, token.lemma_))\n",
        "# no requirement of creating instances\n",
        "#There is no stemming method in Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juM4FwChQ4oO",
        "outputId": "99e838ee-0d0c-4faf-81a0-d9b85fded7ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['he > he', 'was > wa', 'running > running', 'late > late']"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Lemmatization using NLTK\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wn=WordNetLemmatizer()\n",
        "doc =\"he was running late\"\n",
        "['{} > {}'.format(word, wn.lemmatize(word)) for word in doc.split()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEDeCErGc2oB",
        "outputId": "f99f6b59-bec3-4843-da3e-8938c50a2fdb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['he', 'wa', 'run', 'late']"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Lemmatization using NLTK\n",
        "from nltk.stem import PorterStemmer\n",
        "ps=PorterStemmer()\n",
        "doc =\"he was running late\"\n",
        "[ps.stem(word) for word in doc.split()]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23ue5Rfid0Lk"
      },
      "source": [
        "**5. Categorizing words: POS Tagging**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "265PPL3udB42",
        "outputId": "9242d840-00db-49b0-8197-a7db78530159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mary PROPN\n",
            "slapped VERB\n",
            "the DET\n",
            "green ADJ\n",
            "witch NOUN\n",
            ". PUNCT\n"
          ]
        }
      ],
      "source": [
        "#using spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(u\"Mary slapped the green witch.\")\n",
        "for token in doc:\n",
        "  print('{} {}'.format(token, token.pos_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FD_-Oohd7FW",
        "outputId": "672258eb-71b6-4086-f1d0-bdc3597d74c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Mary', 'NNP'),\n",
              " ('slapped', 'VBD'),\n",
              " ('the', 'DT'),\n",
              " ('green', 'JJ'),\n",
              " ('witch', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#using NLTK\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "doc = \"Mary slapped the green witch.\"\n",
        "pos_tag(word_tokenize(doc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugyw8CnwCXhx"
      },
      "source": [
        "**6. Categorizing Spans: Chunking and Named Entity Recognition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DSC7Z3cCdXI"
      },
      "source": [
        "**Chunking:** Often, we need to label a span of text; that is, a contiguous multitoken boundary. For example,\n",
        "consider the sentence, “Mary slapped the green witch.” We might want to identify the noun phrases\n",
        "(NP) and verb phrases (VP) in it,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w44wccBCcJa",
        "outputId": "23230e14-7e10-49ed-acb9-acbd4561d0cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mary NP\n",
            "the green witch NP\n"
          ]
        }
      ],
      "source": [
        "#using spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"Mary slapped the green witch.\")\n",
        "for chunk in doc.noun_chunks:\n",
        "  print ('{} {}'.format(chunk, chunk.label_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kXoOrd-EnuB"
      },
      "source": [
        "**Named Entity**:  A named entity is a string mention of a real\n",
        "world concept like a person, location, organization, drug name, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztSPuF3_C7_w",
        "outputId": "0e90c162-110b-4a6b-84a5-4461d41a5480"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Larry Page', 'PERSON'), ('Google', 'ORG')]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#using spacy\n",
        "doc = nlp(\"Larry Page founded Google\")\n",
        "# Text and label of named entity span\n",
        "[(ent.text, ent.label_) for ent in doc.ents]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeuX5rQqEjdd",
        "outputId": "8da2c5a2-044a-44ca-a427-6925d26a4bd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('NASA', 'NNP'), ('awarded', 'VBD'), ('Elon', 'NNP'), ('Musk', 'NNP'), ('’', 'NNP'), ('s', 'VBD'), ('SpaceX', 'NNP'), ('a', 'DT'), ('$', '$'), ('2.9', 'CD'), ('billion', 'CD'), ('contract', 'NN'), ('to', 'TO'), ('build', 'VB'), ('the', 'DT'), ('lunar', 'NN'), ('lander', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "#using NLTK\n",
        "import nltk\n",
        "from nltk import word_tokenize,pos_tag\n",
        "from nltk import ne_chunk\n",
        "text = \"NASA awarded Elon Musk’s SpaceX a $2.9 billion contract to build the lunar lander.\"\n",
        "tokens = word_tokenize(text)\n",
        "tag=pos_tag(tokens)\n",
        "print(tag)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     /Users/paridhimaheshwari/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.chunk import ne_chunk\n",
        "# nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (ORGANIZATION NASA/NNP)\n",
            "  awarded/VBD\n",
            "  (PERSON Elon/NNP Musk/NNP)\n",
            "  ’/NNP\n",
            "  s/VBD\n",
            "  (ORGANIZATION SpaceX/NNP)\n",
            "  a/DT\n",
            "  $/$\n",
            "  2.9/CD\n",
            "  billion/CD\n",
            "  contract/NN\n",
            "  to/TO\n",
            "  build/VB\n",
            "  the/DT\n",
            "  lunar/NN\n",
            "  lander/NN\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ne_tree = nltk.ne_chunk(tag)\n",
        "print(ne_tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
